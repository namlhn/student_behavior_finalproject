{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1acb98b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import insightface\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "VIDEO_INPUT_PATH = \"assets/input_test.mp4\" # Replace with your video path\n",
    "VIDEO_OUTPUT_PATH = \"assets/output_test_demo.mp4\"\n",
    "\n",
    "# Model paths (ensure files exist)\n",
    "YOLO_MODEL_PATH = \"models/yolov8_best.pt\"\n",
    "EMOTION_MODEL_PATH = \"models/best_resnet18_sgd.ckpt\"\n",
    "\n",
    "# Device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "✅ YOLOv8 Behavior model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\000Mine\\student_behavior_web\\.venv\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:123: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\namlh/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\namlh/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\namlh/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\namlh/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\namlh/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "✅ InsightFace loaded.\n",
      "✅ ResNet18 Emotion model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading models...\")\n",
    "\n",
    "# 1. Load YOLOv8 (Behavior)\n",
    "try:\n",
    "    behavior_model = YOLO(YOLO_MODEL_PATH)\n",
    "    print(\"✅ YOLOv8 Behavior model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading YOLO: {e}\")\n",
    "    behavior_model = None\n",
    "\n",
    "# 2. Load InsightFace (Face Detection)\n",
    "try:\n",
    "    # providers=['CUDAExecutionProvider'] if using GPU and onnxruntime-gpu is installed\n",
    "    # if error, switch to ['CPUExecutionProvider']\n",
    "    identity_model = insightface.app.FaceAnalysis(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "    identity_model.prepare(ctx_id=0 if str(device) == 'cuda' else -1, det_size=(640, 640))\n",
    "    print(\"✅ InsightFace loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading InsightFace: {e}\")\n",
    "    identity_model = None\n",
    "\n",
    "# 3. Load ResNet18 (Emotion)\n",
    "emotion_classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "try:\n",
    "    emotion_model = models.resnet18(weights=None)\n",
    "    emotion_model.fc = torch.nn.Linear(emotion_model.fc.in_features, 7)\n",
    "    \n",
    "    ckpt = torch.load(EMOTION_MODEL_PATH, map_location=device)\n",
    "    # Handle if checkpoint is a dict or direct state_dict\n",
    "    state_dict = ckpt.get(\"model\", ckpt) \n",
    "    emotion_model.load_state_dict(state_dict)\n",
    "    \n",
    "    emotion_model.to(device)\n",
    "    emotion_model.eval()\n",
    "    \n",
    "    # Transform for ResNet\n",
    "    emotion_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    print(\"✅ ResNet18 Emotion model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading Emotion model: {e}\")\n",
    "    emotion_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(face_img_bgr):\n",
    "    if emotion_model is None:\n",
    "        return \"N/A\"\n",
    "    try:\n",
    "        # Convert BGR (OpenCV) -> RGB (PIL)\n",
    "        img_rgb = cv2.cvtColor(face_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(img_rgb)\n",
    "        \n",
    "        # Transform & Inference\n",
    "        input_tensor = emotion_transform(pil_img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = emotion_model(input_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            idx = predicted.item()\n",
    "            return emotion_classes[idx]\n",
    "    except Exception as e:\n",
    "        print(f\"Emotion error: {e}\")\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e6c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video... (1280x720 @ 24.0fps)\n",
      "Processed 30 frames...\n",
      "Processed 60 frames...\n",
      "Processed 90 frames...\n",
      "Processed 120 frames...\n",
      "Processed 150 frames...\n",
      "Processed 180 frames...\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     91\u001b[39m cap.release()\n\u001b[32m     92\u001b[39m out.release()\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdestroyAllWindows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ DONE! Video saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVIDEO_OUTPUT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"
     ]
    }
   ],
   "source": [
    "# Open video\n",
    "cap = cv2.VideoCapture(VIDEO_INPUT_PATH)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Cannot open video: {VIDEO_INPUT_PATH}\")\n",
    "else:\n",
    "    # Get video properties for writer\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Video Writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(VIDEO_OUTPUT_PATH, fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_count = 0\n",
    "    print(f\"Processing video... ({width}x{height} @ {fps}fps)\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Copy frame for drawing\n",
    "        vis_frame = frame.copy()\n",
    "        \n",
    "        # --- STEP 1: DETECT BEHAVIOR (YOLO) ---\n",
    "        if behavior_model:\n",
    "            results = behavior_model(frame, verbose=False, conf=0.4) # conf threshold 0.4\n",
    "            \n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    # Behavior Box coordinates (Global)\n",
    "                    bx1, by1, bx2, by2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                    label = behavior_model.names[int(box.cls[0])]\n",
    "                    \n",
    "                    # Draw Behavior box (Green)\n",
    "                    cv2.rectangle(vis_frame, (bx1, by1), (bx2, by2), (0, 255, 0), 2)\n",
    "                    cv2.putText(vis_frame, f\"Behavior: {label}\", (bx1, by1 - 10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                    \n",
    "                    # --- STEP 2: CROP & DETECT FACE ---\n",
    "                    # Safe crop (avoid border errors)\n",
    "                    h_img, w_img = frame.shape[:2]\n",
    "                    bx1, by1 = max(0, bx1), max(0, by1)\n",
    "                    bx2, by2 = min(w_img, bx2), min(h_img, by2)\n",
    "                    \n",
    "                    if bx2 <= bx1 or by2 <= by1: continue\n",
    "                    \n",
    "                    behavior_crop = frame[by1:by2, bx1:bx2]\n",
    "                    \n",
    "                    if identity_model:\n",
    "                        faces = identity_model.get(behavior_crop)\n",
    "                        \n",
    "                        for face in faces:\n",
    "                            # Face coordinates (Local in behavior_crop)\n",
    "                            fx1, fy1, fx2, fy2 = face.bbox.astype(int)\n",
    "                            \n",
    "                            # --- STEP 3: EMOTION ---\n",
    "                            # Crop face for Emotion Model\n",
    "                            # Clamp local coordinates\n",
    "                            fh, fw = behavior_crop.shape[:2]\n",
    "                            fx1_c, fy1_c = max(0, fx1), max(0, fy1)\n",
    "                            fx2_c, fy2_c = min(fw, fx2), min(fh, fy2)\n",
    "                            \n",
    "                            emotion_label = \"unknown\"\n",
    "                            if fx2_c > fx1_c and fy2_c > fy1_c:\n",
    "                                face_img = behavior_crop[fy1_c:fy2_c, fx1_c:fx2_c]\n",
    "                                emotion_label = predict_emotion(face_img)\n",
    "                            \n",
    "                            # --- STEP 4: DRAW RESULTS (GLOBAL COORDINATES) ---\n",
    "                            # Convert: Global = Behavior_Box + Local_Face\n",
    "                            g_fx1, g_fy1 = bx1 + fx1, by1 + fy1\n",
    "                            g_fx2, g_fy2 = bx1 + fx2, by1 + fy2\n",
    "                            \n",
    "                            # Draw Face box (Red)\n",
    "                            cv2.rectangle(vis_frame, (g_fx1, g_fy1), (g_fx2, g_fy2), (0, 0, 255), 2)\n",
    "                            \n",
    "                            # Display info: Emotion - Unknown Student\n",
    "                            info_text = f\"{emotion_label} | Student: Unknown\"\n",
    "                            cv2.putText(vis_frame, info_text, (g_fx1, g_fy2 + 20), \n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "        # Write frame to output video\n",
    "        out.write(vis_frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "        if frame_count % 30 == 0:\n",
    "            print(f\"Processed {frame_count} frames...\")\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    # Safe destroy windows (headless environments may lack GUI support)\n",
    "    try:\n",
    "        cv2.destroyAllWindows()\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping cv2.destroyAllWindows(): {e}\")\n",
    "    print(f\"✅ DONE! Video saved to: {VIDEO_OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "student_behavior_web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
